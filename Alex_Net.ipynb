{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFgPBmI8BDhi",
        "outputId": "2bee6770-947f-4caa-f977-ea685c219adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import numpy as np \n",
        "import random\n",
        "from numpy import savetxt\n",
        "from numpy import loadtxt\n",
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import glob\n",
        "import cv2\n",
        "from zipfile import ZipFile\n",
        "if(not os.path.exists(\"/content/gdrive/MyDrive/Images\")):\n",
        "  with ZipFile('/content/gdrive/MyDrive/NNDataset.zip','r') as zipObj:\n",
        "    zipObj.extractall('/content/gdrive/MyDrive/Images')\n",
        "if(not os.path.exists(\"/content/gdrive/MyDrive/testImages\")):\n",
        "  with ZipFile('/content/gdrive/MyDrive/data.zip','r') as zipObj:\n",
        "    zipObj.extractall('/content/gdrive/MyDrive/testImages')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# augmantation\n",
        "img_size=227\n",
        "def augmantation(train_dir,img,label):\n",
        "  path =  os.path.join(train_dir,img)\n",
        "  image = cv2.imread(path)\n",
        "  image = cv2.resize(image,(img_size,img_size))  \n",
        "  image = image.reshape(img_size,img_size,3)  \n",
        "  # Main\n",
        "  train_images.append([np.array(image,dtype=object)])\n",
        "  train_labels.append(label)\n",
        "  # T1\n",
        "  im1 = tf.image.flip_left_right(image)\n",
        "  train_images.append([np.array(im1,dtype=object)])\n",
        "  train_labels.append(label)\n",
        "  # T2\n",
        "  im2 = tf.image.flip_up_down(image)\n",
        "  train_images.append([np.array(im2,dtype=object)])\n",
        "  train_labels.append(label)\n",
        "  # T3\n",
        "  im3 = tf.image.flip_left_right(image)\n",
        "  im4 = tf.image.flip_up_down(im3)\n",
        "  train_images.append([np.array(im4,dtype=object)])\n",
        "  train_labels.append(label)\n",
        "  "
      ],
      "metadata": {
        "id": "PMpMFdp-lnyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepairing train_images & train_labels\n",
        "train_dir = \"/content/gdrive/MyDrive/Images/Train\"\n",
        "train_images = [] \n",
        "train_labels =[]\n",
        "classesCount = [0,0,0,0,0,0]\n",
        "if (os.path.exists('/content/gdrive/MyDrive/pre_saved_files/train_images.npy')):  \n",
        "  train_images = np.load('/content/gdrive/MyDrive/pre_saved_files/train_images.npy')\n",
        "  train_labels = np.load('/content/gdrive/MyDrive/pre_saved_files/train_labels.npy')\n",
        "else:\n",
        "  for img in os.listdir(train_dir):\n",
        "    name = img.split('_')[0]\n",
        "    if(name == 'Basketball' and classesCount[0]<180 ):\n",
        "      classesCount[0] = classesCount[0]+1\n",
        "      augmantation(train_dir,img,[0])\n",
        "    if(name == 'Football'and classesCount[1]<180):\n",
        "      classesCount[1] = classesCount[1]+1\n",
        "      augmantation(train_dir,img,[1])\n",
        "    if(name == 'Rowing' and classesCount[2]<180):\n",
        "      classesCount[2] = classesCount[2]+1\n",
        "      augmantation(train_dir,img,[2])\n",
        "    if(name == 'Swimming' and classesCount[3]<180):\n",
        "      classesCount[3] = classesCount[3]+1\n",
        "      augmantation(train_dir,img,[3])\n",
        "    if(name == 'Tennis' and classesCount[4]<180):\n",
        "      classesCount[4] = classesCount[4]+1\n",
        "      augmantation(train_dir,img,[4])\n",
        "    if(name == 'Yoga' and classesCount[5]<180):\n",
        "      classesCount[5] = classesCount[5]+1\n",
        "      augmantation(train_dir,img,[5])\n",
        "  temp = list(zip(train_images, train_labels))\n",
        "  random.shuffle(temp)\n",
        "  train_images, train_labels = zip(*temp)\n",
        "  test_images = train_images[:700]\n",
        "  test_labels = train_labels[:700]\n",
        "  train_images = np.array([i for i in train_images[700:]]).reshape(-1,img_size,img_size,3)\n",
        "  train_labels = np.array([i for i in train_labels[700:]])\n",
        "  train_images = np.asarray(train_images).astype('uint8')\n",
        "  train_labels = np.asarray(train_labels).astype('uint8')\n",
        "  np.save('/content/gdrive/MyDrive/pre_saved_files/train_images.npy', train_images)  \n",
        "  np.save('/content/gdrive/MyDrive/pre_saved_files/train_labels.npy', train_labels)\n",
        "print(train_images.shape)\n",
        "print(train_labels.shape)  \n"
      ],
      "metadata": {
        "id": "MDUqEMIjlo-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22553332-f096-4f6d-9311-fd08c62c4a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3620, 227, 227, 3)\n",
            "(3620, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for prepairing test_images & test_labels\n",
        "\n",
        "# test_images = []\n",
        "# test_labels = []\n",
        "if (os.path.exists('/content/gdrive/MyDrive/pre_saved_files/test_images.npy')):  \n",
        "  test_images = np.load('/content/gdrive/MyDrive/pre_saved_files/test_images.npy')\n",
        "  test_labels = np.load('/content/gdrive/MyDrive/pre_saved_files/test_labels.npy')\n",
        "else:  \n",
        "  test_images = np.array([i for i in test_images]).reshape(-1,img_size,img_size,3)\n",
        "  test_labels = np.array([i for i in test_labels])\n",
        "  test_images = np.asarray(test_images).astype('uint8')\n",
        "  test_labels = np.asarray(test_labels).astype('uint8')\n",
        "  np.save('/content/gdrive/MyDrive/pre_saved_files/test_images.npy', test_images)  \n",
        "  np.save('/content/gdrive/MyDrive/pre_saved_files/test_labels.npy', test_labels)\n",
        "print(test_images.shape)\n",
        "print(test_labels.shape)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2bWHmiglqvJ",
        "outputId": "61418693-345e-4b0a-f4ff-573ebccac112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(700, 227, 227, 3)\n",
            "(700, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds=tf.data.Dataset.from_tensor_slices((train_images,train_labels))\n",
        "test_ds=tf.data.Dataset.from_tensor_slices((test_images,test_labels))\n",
        "train_ds_size=tf.data.experimental.cardinality(train_ds).numpy()\n",
        "test_ds_size=tf.data.experimental.cardinality(test_ds).numpy()\n",
        "print('Train size:',train_ds_size)\n",
        "print('Test size:',test_ds_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9b57ZYElsnR",
        "outputId": "ad79b432-aa53-48a7-b320-a9a686dc862b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 3620\n",
            "Test size: 700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds=(train_ds          \n",
        "           .shuffle(buffer_size=train_ds_size)\n",
        "           .batch(batch_size=32,drop_remainder=True)\n",
        "         )\n",
        "test_ds=(test_ds         \n",
        "          .shuffle(buffer_size=test_ds_size)\n",
        "          .batch(batch_size=32,drop_remainder=True)\n",
        "         )"
      ],
      "metadata": {
        "id": "LUpcFtjdluQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (os.path.exists('/content/gdrive/MyDrive/pre_saved_files/saved_model7')):  \n",
        "  Loaded_model = keras.models.load_model(\"/content/gdrive/MyDrive/pre_saved_files/saved_model7\")\n",
        "else:\n",
        "  model=keras.models.Sequential([\n",
        "      keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(img_size,img_size,3)),\n",
        "      keras.layers.BatchNormalization(),\n",
        "      keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "      keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "      keras.layers.BatchNormalization(),\n",
        "      keras.layers.MaxPool2D(pool_size=(3,3)),\n",
        "      keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "      keras.layers.BatchNormalization(),\n",
        "      keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "      keras.layers.BatchNormalization(),\n",
        "      keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "      keras.layers.BatchNormalization(),\n",
        "      keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "      keras.layers.Flatten(),\n",
        "      keras.layers.Dense(4096,activation='relu'),\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.Dense(4096,activation='relu'),\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.Dense(6,activation='softmax')   \n",
        "  ])\n",
        "  model.compile(\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      optimizer=tf.optimizers.SGD(lr=0.001),\n",
        "      metrics=['accuracy']    \n",
        "  )\n",
        "  model.summary() \n",
        "  history=model.fit(\n",
        "      train_ds,\n",
        "      epochs=50,\n",
        "      validation_data=test_ds,\n",
        "      validation_freq=1,     \n",
        "      )\n",
        "  \n",
        "  model.save('/content/gdrive/MyDrive/pre_saved_files/saved_model7',save_format=\"h5\")"
      ],
      "metadata": {
        "id": "Vbiktp6tlvuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = Loaded_model.evaluate(test_images, test_labels, verbose=2)\n",
        "print(\"accuracy: {:.2f}%\".format(100 * acc))"
      ],
      "metadata": {
        "id": "09s7OBbNlyHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7623362-ce39-43cd-9670-cfd962a70618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 - 1s - loss: 0.3725 - accuracy: 0.8757 - 797ms/epoch - 36ms/step\n",
            "accuracy: 87.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = \"/content/gdrive/MyDrive/Images/Test\"\n",
        "test_images_names=[]\n",
        "test_images2 = []\n",
        "if (os.path.exists('/content/gdrive/MyDrive/pre_saved_files/test_images2.npy')):  \n",
        "  test_images2 = np.load('/content/gdrive/MyDrive/pre_saved_files/test_images2.npy')\n",
        "  for img in os.listdir(test_dir):\n",
        "    test_images_names.append(img)\n",
        "else:\n",
        "  for img in os.listdir(test_dir):\n",
        "    test_images_names.append(img)\n",
        "    path =  os.path.join(test_dir,img)\n",
        "    image = cv2.imread(path)\n",
        "    image = cv2.resize(image,(img_size,img_size)) \n",
        "    test_images2.append([np.array(image,dtype=object)])    \n",
        "  test_images2 = np.array([i for i in test_images2]).reshape(-1,img_size,img_size,3)\n",
        "  test_images2 = np.asarray(test_images2).astype('uint8')\n",
        "  np.save('/content/gdrive/MyDrive/pre_saved_files/test_images2.npy', test_images2)  \n",
        "print(test_images2.shape)"
      ],
      "metadata": {
        "id": "P_tQVCXBl3yJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "011b25a6-4019-427d-ec95-115c60f709d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(688, 227, 227, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted =Loaded_model.predict(test_images2)"
      ],
      "metadata": {
        "id": "rdn73qzil5Sx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530bc767-3341-47d3-be3a-cf8cb6cf0ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 24ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [0,1,2,3,4,5]\n",
        "CLASS_NAMES= ['Basketball', 'Football', 'Rowing', 'Swimming', 'Tennis', 'Yoga']\n",
        "data =[]\n",
        "for i in range(len(predicted)):\n",
        "  mxindex = -1\n",
        "  mx =0\n",
        "  for j in range(6):\n",
        "    if(predicted[i][j] > mx):\n",
        "      mxindex = j\n",
        "      mx = predicted[i][j]\n",
        "  data.append([test_images_names[i],classes[mxindex]])\n",
        "Datapd=pd.DataFrame(data)\n",
        "Datapd.columns=['image_name','Label']\n",
        "Datapd.to_csv('predictions.csv',index=False)\n"
      ],
      "metadata": {
        "id": "yM3M2guAl7Vi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}